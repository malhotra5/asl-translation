{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language to Speech Translation System\n",
    "\n",
    "This notebook implements a complete pipeline for sign language translation:\n",
    "1. Sign language video to text translation\n",
    "2. Grammatical error correction using an LLM\n",
    "3. Text to speech conversion using ESPnet\n",
    "\n",
    "Let's start by installing the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers datasets evaluate\n",
    "!pip install opencv-python mediapipe\n",
    "!pip install openai\n",
    "!pip install espnet espnet_model_zoo\n",
    "!pip install soundfile\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sign Language to Text Translation\n",
    "\n",
    "For this part, we'll use the WLASL dataset (Word-Level American Sign Language) and implement a model to recognize signs from video frames. We'll use MediaPipe for pose estimation and a sequence model to classify the signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import gdown\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download and Prepare the WLASL Dataset\n",
    "\n",
    "We'll use a subset of the WLASL dataset for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create directories for dataset\n",
    "!mkdir -p data/wlasl\n",
    "\n",
    "# Download WLASL dataset metadata\n",
    "!wget -O data/wlasl/WLASL_v0.3.json https://raw.githubusercontent.com/dxli94/WLASL/master/start_kit/WLASL_v0.3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the WLASL metadata\n",
    "with open('data/wlasl/WLASL_v0.3.json', 'r') as f:\n",
    "    wlasl_data = json.load(f)\n",
    "\n",
    "print(f\"Total number of glosses (words): {len(wlasl_data)}\")\n",
    "\n",
    "# For demonstration, we'll use a subset of the data (first 10 words)\n",
    "subset_data = wlasl_data[:10]\n",
    "print(f\"Using a subset of {len(subset_data)} words\")\n",
    "\n",
    "# Print the words in our subset\n",
    "words = [item['gloss'] for item in subset_data]\n",
    "print(f\"Words in our subset: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to download videos from the dataset\n",
    "def download_wlasl_videos(data, output_dir='data/wlasl/videos'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Keep track of videos and their labels\n",
    "    video_paths = []\n",
    "    video_labels = []\n",
    "    label_to_gloss = {}\n",
    "    \n",
    "    for label_idx, entry in enumerate(tqdm(data, desc=\"Downloading videos\")):\n",
    "        gloss = entry['gloss']\n",
    "        label_to_gloss[label_idx] = gloss\n",
    "        \n",
    "        for instance in entry['instances']:\n",
    "            video_id = instance['video_id']\n",
    "            video_url = instance.get('url', '')\n",
    "            \n",
    "            # Skip if no URL is provided\n",
    "            if not video_url:\n",
    "                continue\n",
    "                \n",
    "            # Define output path\n",
    "            output_path = os.path.join(output_dir, f\"{video_id}.mp4\")\n",
    "            \n",
    "            # Skip if already downloaded\n",
    "            if os.path.exists(output_path):\n",
    "                video_paths.append(output_path)\n",
    "                video_labels.append(label_idx)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Download the video\n",
    "                response = requests.get(video_url, stream=True, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    with open(output_path, 'wb') as f:\n",
    "                        for chunk in response.iter_content(chunk_size=1024):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                    video_paths.append(output_path)\n",
    "                    video_labels.append(label_idx)\n",
    "                else:\n",
    "                    print(f\"Failed to download {video_id}: HTTP {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {video_id}: {e}\")\n",
    "    \n",
    "    return video_paths, video_labels, label_to_gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download the videos for our subset\n",
    "video_paths, video_labels, label_to_gloss = download_wlasl_videos(subset_data)\n",
    "\n",
    "print(f\"Downloaded {len(video_paths)} videos\")\n",
    "print(f\"Label to gloss mapping: {label_to_gloss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implement MediaPipe for Pose Estimation\n",
    "\n",
    "We'll use MediaPipe to extract hand landmarks and pose information from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize MediaPipe solutions\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def extract_landmarks(video_path, max_frames=30):\n",
    "    \"\"\"Extract pose, face, and hand landmarks from a video using MediaPipe.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = frame_count / fps\n",
    "    \n",
    "    # Calculate frames to sample\n",
    "    if frame_count <= max_frames:\n",
    "        frames_to_sample = list(range(frame_count))\n",
    "    else:\n",
    "        frames_to_sample = np.linspace(0, frame_count-1, max_frames, dtype=int)\n",
    "    \n",
    "    landmarks_sequence = []\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for frame_idx in frames_to_sample:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            success, image = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                # If frame read failed, append zeros\n",
    "                landmarks_sequence.append(np.zeros((1, 543)))\n",
    "                continue\n",
    "                \n",
    "            # Convert the BGR image to RGB\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process the image and detect landmarks\n",
    "            results = holistic.process(image)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            frame_landmarks = []\n",
    "            \n",
    "            # Pose landmarks (33 landmarks x 3 coordinates)\n",
    "            if results.pose_landmarks:\n",
    "                pose = [[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]\n",
    "                frame_landmarks.extend(np.array(pose).flatten())\n",
    "            else:\n",
    "                frame_landmarks.extend(np.zeros(33*3))\n",
    "                \n",
    "            # Left hand landmarks (21 landmarks x 3 coordinates)\n",
    "            if results.left_hand_landmarks:\n",
    "                left_hand = [[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]\n",
    "                frame_landmarks.extend(np.array(left_hand).flatten())\n",
    "            else:\n",
    "                frame_landmarks.extend(np.zeros(21*3))\n",
    "                \n",
    "            # Right hand landmarks (21 landmarks x 3 coordinates)\n",
    "            if results.right_hand_landmarks:\n",
    "                right_hand = [[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]\n",
    "                frame_landmarks.extend(np.array(right_hand).flatten())\n",
    "            else:\n",
    "                frame_landmarks.extend(np.zeros(21*3))\n",
    "                \n",
    "            # Face landmarks (we'll use a subset of 10 landmarks for simplicity)\n",
    "            if results.face_landmarks:\n",
    "                # Select a subset of face landmarks (e.g., eyes, nose, mouth)\n",
    "                face_indices = [0, 4, 6, 8, 10, 152, 234, 454, 10, 338]  # Example indices\n",
    "                face = [[results.face_landmarks.landmark[idx].x,\n",
    "                         results.face_landmarks.landmark[idx].y,\n",
    "                         results.face_landmarks.landmark[idx].z] for idx in face_indices]\n",
    "                frame_landmarks.extend(np.array(face).flatten())\n",
    "            else:\n",
    "                frame_landmarks.extend(np.zeros(10*3))\n",
    "                \n",
    "            landmarks_sequence.append(frame_landmarks)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Pad or truncate to ensure all sequences have the same length\n",
    "    if len(landmarks_sequence) < max_frames:\n",
    "        # Pad with zeros\n",
    "        pad_length = max_frames - len(landmarks_sequence)\n",
    "        landmarks_sequence.extend([np.zeros_like(landmarks_sequence[0])] * pad_length)\n",
    "    \n",
    "    return np.array(landmarks_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to visualize the landmarks\n",
    "def visualize_landmarks(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        success, image = cap.read()\n",
    "        \n",
    "        if not success:\n",
    "            print(\"Failed to read video\")\n",
    "            return\n",
    "            \n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the image and detect landmarks\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        image_copy = image.copy()\n",
    "        \n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_copy, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        \n",
    "        # Draw left hand landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_copy, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        \n",
    "        # Draw right hand landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_copy, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        \n",
    "        # Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_copy, results.face_landmarks)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image_copy)\n",
    "        plt.title(\"MediaPipe Landmarks\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize landmarks for the first video\n",
    "if video_paths:\n",
    "    visualize_landmarks(video_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create a Dataset and DataLoader for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transform=None, max_frames=30):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Extract landmarks\n",
    "        landmarks = extract_landmarks(video_path, self.max_frames)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        landmarks_tensor = torch.tensor(landmarks, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return landmarks_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    video_paths, video_labels, test_size=0.2, random_state=42, stratify=video_labels\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_paths)} videos\")\n",
    "print(f\"Test set: {len(test_paths)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create datasets\n",
    "train_dataset = SignLanguageDataset(train_paths, train_labels)\n",
    "test_dataset = SignLanguageDataset(test_paths, test_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Define the Sign Language Recognition Model\n",
    "\n",
    "We'll use a combination of LSTM and fully connected layers to classify the sign language gestures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SignLanguageModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(SignLanguageModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # *2 for bidirectional\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        \n",
    "        # LSTM output: (batch_size, sequence_length, hidden_dim * 2)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(lstm_out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the model\n",
    "input_dim = 543  # Total number of features per frame (pose + hands + face landmarks)\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "num_classes = len(label_to_gloss)\n",
    "\n",
    "model = SignLanguageModel(input_dim, hidden_dim, num_layers, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model\n",
    "model = train_model(model, train_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "accuracy, all_preds, all_labels = evaluate_model(model, test_loader)\n",
    "\n",
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[label_to_gloss[i] for i in range(len(label_to_gloss))],\n",
    "            yticklabels=[label_to_gloss[i] for i in range(len(label_to_gloss))])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[label_to_gloss[i] for i in range(len(label_to_gloss))]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'label_to_gloss': label_to_gloss,\n",
    "    'input_dim': input_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'num_classes': num_classes\n",
    "}, 'sign_language_model.pth')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Function to Translate Sign Language Videos to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def translate_sign_to_text(video_path, model, label_to_gloss):\n",
    "    \"\"\"Translate a sign language video to text.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract landmarks\n",
    "    landmarks = extract_landmarks(video_path)\n",
    "    landmarks_tensor = torch.tensor(landmarks, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(landmarks_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_label = predicted.item()\n",
    "    \n",
    "    # Convert to text\n",
    "    predicted_text = label_to_gloss[predicted_label]\n",
    "    \n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the translation function on a sample video\n",
    "if test_paths:\n",
    "    sample_video = test_paths[0]\n",
    "    predicted_text = translate_sign_to_text(sample_video, model, label_to_gloss)\n",
    "    print(f\"Predicted sign: {predicted_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Grammatical Error Correction using LLM\n",
    "\n",
    "Now, we'll use an LLM to correct any grammatical errors in the translated text. We'll use OpenAI's GPT model for this purpose, but you can also use an open-source alternative like T5 or BART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import OpenAI library\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# client = OpenAI(api_key=\"your-api-key\")\n",
    "\n",
    "# For demonstration, we'll use a mock function\n",
    "def correct_grammar_with_llm(text):\n",
    "    \"\"\"Correct grammatical errors in the text using an LLM.\"\"\"\n",
    "    # In a real implementation, you would use the OpenAI API like this:\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"gpt-3.5-turbo\",\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": \"You are a helpful assistant that corrects grammatical errors in text.\"},\n",
    "    #         {\"role\": \"user\", \"content\": f\"Correct any grammatical errors in the following text: '{text}'\"},\n",
    "    #     ]\n",
    "    # )\n",
    "    # corrected_text = response.choices[0].message.content\n",
    "    \n",
    "    # For demonstration, we'll use a simple mock function\n",
    "    print(f\"Original text: {text}\")\n",
    "    \n",
    "    # Simulate some common ASL to English grammar corrections\n",
    "    # ASL often omits articles and uses different word order\n",
    "    if text.lower() == \"book on table\":\n",
    "        corrected_text = \"The book is on the table.\"\n",
    "    elif text.lower() == \"me go store\":\n",
    "        corrected_text = \"I am going to the store.\"\n",
    "    elif text.lower() == \"yesterday me sick\":\n",
    "        corrected_text = \"I was sick yesterday.\"\n",
    "    else:\n",
    "        # Add articles and proper verb forms as a simple example\n",
    "        words = text.split()\n",
    "        if len(words) > 0 and words[0].lower() not in [\"the\", \"a\", \"an\", \"i\", \"you\", \"he\", \"she\", \"we\", \"they\"]:\n",
    "            words.insert(0, \"The\")\n",
    "        corrected_text = \" \".join(words)\n",
    "    \n",
    "    print(f\"Corrected text: {corrected_text}\")\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Alternative: Use Hugging Face's T5 model for grammar correction\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def load_grammar_correction_model():\n",
    "    \"\"\"Load a pre-trained T5 model for grammar correction.\"\"\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"prithivida/grammar_error_correcter_v1\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"prithivida/grammar_error_correcter_v1\")\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def correct_grammar_with_t5(text, model, tokenizer):\n",
    "    \"\"\"Correct grammatical errors in the text using T5.\"\"\"\n",
    "    input_text = f\"grammar: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(input_ids, max_length=128)\n",
    "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Original text: {text}\")\n",
    "    print(f\"Corrected text: {corrected_text}\")\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the T5 model for grammar correction\n",
    "grammar_model, grammar_tokenizer = load_grammar_correction_model()\n",
    "\n",
    "# Test the grammar correction function\n",
    "sample_texts = [\n",
    "    \"book on table\",\n",
    "    \"me go store\",\n",
    "    \"yesterday me sick\",\n",
    "    \"she eat apple\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    # Using the mock function\n",
    "    print(\"\\nUsing mock function:\")\n",
    "    corrected_text_mock = correct_grammar_with_llm(text)\n",
    "    \n",
    "    # Using T5\n",
    "    print(\"\\nUsing T5 model:\")\n",
    "    corrected_text_t5 = correct_grammar_with_t5(text, grammar_model, grammar_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text to Speech Conversion using ESPnet\n",
    "\n",
    "Finally, we'll convert the corrected text to speech using ESPnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import ESPnet libraries\n",
    "import soundfile as sf\n",
    "from espnet2.bin.tts_inference import Text2Speech\n",
    "from espnet_model_zoo.downloader import ModelDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download and load a pre-trained TTS model\n",
    "def load_tts_model():\n",
    "    \"\"\"Download and load a pre-trained ESPnet TTS model.\"\"\"\n",
    "    d = ModelDownloader()\n",
    "    model_config = d.download_and_unpack(\"kan-bayashi/ljspeech_tacotron2\")\n",
    "    tts = Text2Speech(**model_config[\"tts_train_args\"])\n",
    "    tts.load_state_dict(d.download_and_unpack(model_config[\"tts_model_file\"]))\n",
    "    return tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to convert text to speech\n",
    "def text_to_speech(text, tts_model, output_path=\"output.wav\"):\n",
    "    \"\"\"Convert text to speech using ESPnet.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        wav = tts_model(text)[\"wav\"]\n",
    "    \n",
    "    sf.write(output_path, wav.numpy(), tts_model.fs, \"PCM_16\")\n",
    "    print(f\"Speech saved to {output_path}\")\n",
    "    \n",
    "    # Play the audio\n",
    "    from IPython.display import Audio\n",
    "    return Audio(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the TTS model\n",
    "tts_model = load_tts_model()\n",
    "\n",
    "# Test the TTS function\n",
    "sample_text = \"The book is on the table.\"\n",
    "audio = text_to_speech(sample_text, tts_model, \"sample_output.wav\")\n",
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Putting It All Together - Complete Pipeline\n",
    "\n",
    "Now, let's create a complete pipeline that takes a sign language video as input and outputs speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def sign_language_to_speech_pipeline(video_path, sign_model, label_to_gloss, grammar_model, grammar_tokenizer, tts_model, output_path=\"output.wav\"):\n",
    "    \"\"\"Complete pipeline from sign language video to speech.\"\"\"\n",
    "    # Step 1: Translate sign language to text\n",
    "    print(\"Step 1: Translating sign language to text...\")\n",
    "    raw_text = translate_sign_to_text(video_path, sign_model, label_to_gloss)\n",
    "    print(f\"Raw text: {raw_text}\")\n",
    "    \n",
    "    # Step 2: Correct grammar\n",
    "    print(\"\\nStep 2: Correcting grammar...\")\n",
    "    corrected_text = correct_grammar_with_t5(raw_text, grammar_model, grammar_tokenizer)\n",
    "    \n",
    "    # Step 3: Convert to speech\n",
    "    print(\"\\nStep 3: Converting to speech...\")\n",
    "    audio = text_to_speech(corrected_text, tts_model, output_path)\n",
    "    \n",
    "    return raw_text, corrected_text, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the complete pipeline\n",
    "if test_paths:\n",
    "    sample_video = test_paths[0]\n",
    "    raw_text, corrected_text, audio = sign_language_to_speech_pipeline(\n",
    "        sample_video, model, label_to_gloss, grammar_model, grammar_tokenizer, tts_model, \"final_output.wav\"\n",
    "    )\n",
    "    audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluation of the Complete System\n",
    "\n",
    "Let's evaluate the performance of our complete system on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_complete_system(test_paths, test_labels, sign_model, label_to_gloss, grammar_model, grammar_tokenizer):\n",
    "    \"\"\"Evaluate the complete system on the test set.\"\"\"\n",
    "    sign_recognition_correct = 0\n",
    "    total = len(test_paths)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (video_path, true_label) in enumerate(zip(test_paths, test_labels)):\n",
    "        print(f\"Processing video {i+1}/{total}...\")\n",
    "        \n",
    "        # Step 1: Translate sign language to text\n",
    "        raw_text = translate_sign_to_text(video_path, sign_model, label_to_gloss)\n",
    "        true_text = label_to_gloss[true_label]\n",
    "        \n",
    "        # Check if sign recognition is correct\n",
    "        sign_correct = (raw_text == true_text)\n",
    "        if sign_correct:\n",
    "            sign_recognition_correct += 1\n",
    "        \n",
    "        # Step 2: Correct grammar\n",
    "        corrected_text = correct_grammar_with_t5(raw_text, grammar_model, grammar_tokenizer)\n",
    "        \n",
    "        results.append({\n",
    "            'video_path': video_path,\n",
    "            'true_label': true_label,\n",
    "            'true_text': true_text,\n",
    "            'raw_text': raw_text,\n",
    "            'corrected_text': corrected_text,\n",
    "            'sign_correct': sign_correct\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    sign_recognition_accuracy = 100 * sign_recognition_correct / total\n",
    "    print(f\"Sign Recognition Accuracy: {sign_recognition_accuracy:.2f}%\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the complete system\n",
    "evaluation_results = evaluate_complete_system(\n",
    "    test_paths[:10],  # Use a subset for demonstration\n",
    "    test_labels[:10],\n",
    "    model,\n",
    "    label_to_gloss,\n",
    "    grammar_model,\n",
    "    grammar_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display the evaluation results\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "results_df = results_df[['true_text', 'raw_text', 'corrected_text', 'sign_correct']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've built a complete sign language to speech translation system with three main components:\n",
    "\n",
    "1. **Sign Language to Text Translation**: We used MediaPipe to extract landmarks from sign language videos and trained an LSTM-based model to recognize signs.\n",
    "\n",
    "2. **Grammatical Error Correction**: We used a T5 model to correct any grammatical errors in the translated text, making it more natural and fluent.\n",
    "\n",
    "3. **Text to Speech Conversion**: We used ESPnet to convert the corrected text to speech, completing the pipeline.\n",
    "\n",
    "This system can be further improved by:\n",
    "- Using a larger and more diverse dataset for sign language recognition\n",
    "- Implementing continuous sign language recognition for full sentences\n",
    "- Fine-tuning the grammar correction model specifically for sign language translation\n",
    "- Using a more advanced TTS model for more natural-sounding speech\n",
    "\n",
    "The system demonstrates the potential of AI to bridge communication gaps between deaf and hearing communities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}